{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf98b6e-3eb1-4a30-b112-bf758a40e9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 years: ['2019', '2017', '2011', '2018', '2015', '2010', '2002', '2007', '2020', '2006', '2013', '2012', '2016', '2021', '2008', '2003', '2004', '2023', '2014', '2022', '2005', '2009']\n",
      "\n",
      "Merging 2002 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2002_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2003 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2003_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2004 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2004_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2005 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2005_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2006 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2006_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2007 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2007_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2008 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2008_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2009 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2009_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2010 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2010_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2011 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2011_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2012 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2012_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2013 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2013_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2014 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2014_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2015 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2015_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2016 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2016_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2017 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2017_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2018 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2018_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2019 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2019_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2020 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2020_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2021 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2021_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2022 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2022_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "Merging 2023 (9 tiles)...\n",
      "Saved: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/ObsCount_2023_Sep23_to_Mar21_MERGED.tif\n",
      "\n",
      "✅ All years merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Merge Antarctic MODIS ObsCount tiles by year.\n",
    "Input : G:/Hangkai/Anttarctic Vegetation Dynamic/MODIS_OBS/Antarctic_ObsCount\n",
    "Output: G:/Hangkai/Anttarctic Vegetation Dynamic/MODIS_OBS/Antarctic_ObsCount/Merged\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "\n",
    "# ================= CONFIG =================\n",
    "base_dir = Path(r\"/mnt/cephfs-mount/hangkai/Antarctic_ObsCount/\")\n",
    "out_dir  = base_dir / \"Merged\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ================= MAIN =================\n",
    "# Find all .tif files\n",
    "tif_files = list(base_dir.glob(\"*.tif\"))\n",
    "\n",
    "# Extract years using regex (e.g., ObsCount_2002_Sep23_to_Mar21-....tif)\n",
    "pattern = re.compile(r\"ObsCount_(\\d{4})_Sep23_to_Mar21\")\n",
    "\n",
    "# Group by year\n",
    "year_groups = {}\n",
    "for tif in tif_files:\n",
    "    match = pattern.search(tif.name)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        year_groups.setdefault(year, []).append(tif)\n",
    "\n",
    "print(f\"Found {len(year_groups)} years:\", list(year_groups.keys()))\n",
    "\n",
    "# Merge per year\n",
    "for year, files in sorted(year_groups.items()):\n",
    "    print(f\"\\nMerging {year} ({len(files)} tiles)...\")\n",
    "    srcs = [rasterio.open(f) for f in files]\n",
    "    mosaic, transform = merge(srcs)\n",
    "    meta = srcs[0].meta.copy()\n",
    "    for s in srcs: s.close()\n",
    "\n",
    "    meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": transform,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    out_path = out_dir / f\"ObsCount_{year}_Sep23_to_Mar21_MERGED.tif\"\n",
    "    with rasterio.open(out_path, \"w\", **meta) as dst:\n",
    "        dst.write(mosaic)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(\"\\n✅ All years merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c0980-f973-468a-ab31-26edd6c3102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Compute mean and standard deviation of clear observations (ObsCount)\n",
    "across years, and save the maps.\n",
    "\n",
    "Input : /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged\n",
    "Output: /mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged/Stats\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import re\n",
    "import pandas as pd\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# ================= CONFIG =================\n",
    "in_dir  = Path(\"/mnt/cephfs-mount/hangkai/Antarctic_ObsCount/Merged\")\n",
    "out_dir = in_dir / \"Stats\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ================= LOAD FILES =================\n",
    "pattern = re.compile(r\"ObsCount_(\\d{4})_Sep23_to_Mar21_MERGED\\.tif$\")\n",
    "tifs = sorted([f for f in in_dir.glob(\"*.tif\") if pattern.search(f.name)])\n",
    "\n",
    "if not tifs:\n",
    "    raise FileNotFoundError(f\"No merged files found in {in_dir}\")\n",
    "\n",
    "years = []\n",
    "stack = []\n",
    "meta = None\n",
    "\n",
    "for fp in tifs:\n",
    "    year = int(pattern.search(fp.name).group(1))\n",
    "    with rasterio.open(fp) as src:\n",
    "        arr = src.read(1, masked=True)\n",
    "        if meta is None:\n",
    "            meta = src.meta.copy()\n",
    "        stack.append(arr)\n",
    "        years.append(year)\n",
    "\n",
    "# Convert to 3D masked array [time, rows, cols]\n",
    "data = np.ma.stack(stack, axis=0)\n",
    "\n",
    "# ================= PER-YEAR STATS =================\n",
    "records = []\n",
    "for i, year in enumerate(years):\n",
    "    arr = data[i]\n",
    "    records.append({\n",
    "        \"year\": year,\n",
    "        \"valid_px\": np.ma.count(arr),\n",
    "        \"mean_obs\": float(arr.mean()),\n",
    "        \"std_obs\": float(arr.std())\n",
    "    })\n",
    "df = pd.DataFrame(records).sort_values(\"year\")\n",
    "print(df)\n",
    "\n",
    "# ================= MEAN & STD MAPS =================\n",
    "mean_map = np.ma.mean(data, axis=0)\n",
    "std_map  = np.ma.std(data, axis=0)\n",
    "\n",
    "# Update metadata\n",
    "meta.update({\n",
    "    \"driver\": \"GTiff\",\n",
    "    \"dtype\": \"float32\",\n",
    "    \"count\": 1,\n",
    "    \"compress\": \"lzw\"\n",
    "})\n",
    "\n",
    "# Save mean map\n",
    "mean_path = out_dir / \"ObsCount_Mean_AcrossYears.tif\"\n",
    "with rasterio.open(mean_path, \"w\", **meta) as dst:\n",
    "    dst.write(mean_map.filled(np.nan).astype(\"float32\"), 1)\n",
    "print(f\"✅ Saved mean map: {mean_path}\")\n",
    "\n",
    "# Save std map\n",
    "std_path = out_dir / \"ObsCount_Std_AcrossYears.tif\"\n",
    "with rasterio.open(std_path, \"w\", **meta) as dst:\n",
    "    dst.write(std_map.filled(np.nan).astype(\"float32\"), 1)\n",
    "print(f\"✅ Saved std map: {std_path}\")\n",
    "\n",
    "# ================= SUMMARY =================\n",
    "print(\"\\n===== SUMMARY OF CLEAR OBS =====\")\n",
    "print(f\"Overall mean of mean-map: {mean_map.mean():.2f}\")\n",
    "print(f\"Overall std  of mean-map: {mean_map.std():.2f}\")\n",
    "print(f\"Overall mean of std-map : {std_map.mean():.2f}\")\n",
    "print(f\"Overall std  of std-map : {std_map.std():.2f}\")\n",
    "\n",
    "# Optional: save yearly summary table\n",
    "df.to_csv(out_dir / \"ObsCount_Yearly_Stats.csv\", index=False)\n",
    "print(f\"✅ Saved yearly stats table: {out_dir / 'ObsCount_Yearly_Stats.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7b0a9-a1d5-4ff6-a6c9-4f3e88a2a66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a105c78-ce9c-4b59-97a7-a9c8e3976055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:46] Mode: LOCAL 3x3 + HALO\n",
      "  Input : /mnt/cephfs-mount/hangkai/2020\n",
      "  Output: /mnt/cephfs-mount/hangkai/2020_depth\n",
      "  Files : 259\n",
      "  Halo (px): 2048\n",
      "  Subtract half pixel: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing tiles: 100%|████████████████████████| 259/259 [00:02<00:00, 93.88tile/s]\n",
      "Processing tiles:  14%|███                   | 36/259 [00:00<00:01, 175.44tile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:48] Skip (exists): 00N_000E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_040E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_040W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_050W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_060W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_070W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_080W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_090E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_090W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_100E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_110E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_120E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_130E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_140E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_150E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 00N_160E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_000E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_010W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_020W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_040E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_050W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_060W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_070E_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_070W_depth_m.tif\n",
      "[2025-09-18 03:12:48] Skip (exists): 10N_080E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10N_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10N_090E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10N_090W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10N_100E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10N_110E_depth_m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  28%|██████                | 72/259 [00:00<00:01, 174.61tile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:49] Skip (exists): 10N_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10N_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_040E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_040W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_050E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_050W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_060W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_110E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_140E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_150E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_160E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 10S_170E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_000E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_010W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_020W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_040E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_050E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_060W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_070E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_080E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_090E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_090W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_100E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_100W_depth_m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  35%|███████▋              | 90/259 [00:00<00:00, 171.77tile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:49] Skip (exists): 20N_110E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_110W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20N_160W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_040E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_050W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_060W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_110E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_140E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_150E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 20S_160E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_000E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_010W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_020W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_040E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_050E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_060E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_070E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_080E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_090E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_090W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_100E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_100W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_110E_depth_m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  49%|██████████▎          | 127/259 [00:00<00:00, 175.70tile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:49] Skip (exists): 30N_110W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_120W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_160W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30N_170W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_060W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_110E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_140E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_150E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 30S_170E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_000E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_010W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_020W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_040E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_050E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_060E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_070E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_080E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_090E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_090W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_100E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_100W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_110E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_110W_depth_m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  64%|█████████████▍       | 165/259 [00:01<00:00, 177.62tile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:49] Skip (exists): 40N_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_120W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_130W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40N_140E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40S_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40S_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40S_140E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40S_160E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 40S_170E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_000E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_010W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_040E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_050E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_060E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_060W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_070E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_080E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_090E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_090W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_100E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_100W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_110E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_110W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_120E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_120W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_130E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_130W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_140E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50N_150E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50S_070W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 50S_080W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 60N_000E_depth_m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  71%|██████████████▊      | 183/259 [00:01<00:00, 178.24tile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:12:49] Skip (exists): 60N_010E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 60N_010W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 60N_020E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 60N_020W_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 60N_030E_depth_m.tif\n",
      "[2025-09-18 03:12:49] Skip (exists): 60N_040E_depth_m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  73%|███████████████▉      | 188/259 [07:14<10:55,  9.23s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:20:03] Done 60N_050E.tif in 433.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  73%|████████████████      | 189/259 [13:43<23:48, 20.40s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:26:31] Done 60N_060E.tif in 388.31 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  73%|████████████████▏     | 190/259 [19:16<38:01, 33.06s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:32:05] Done 60N_060W.tif in 333.69 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  74%|████████████████▏     | 191/259 [25:39<59:06, 52.16s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:38:28] Done 60N_070E.tif in 382.89 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  74%|██████████████▊     | 192/259 [32:02<1:24:55, 76.05s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:44:51] Done 60N_070W.tif in 382.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  75%|██████████████▏    | 193/259 [38:53<1:58:11, 107.45s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:51:42] Done 60N_080E.tif in 411.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  75%|██████████████▏    | 194/259 [44:38<2:26:44, 135.45s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 03:57:27] Done 60N_080W.tif in 344.78 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  75%|██████████████▎    | 195/259 [49:13<2:45:54, 155.54s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:02:01] Done 60N_090E.tif in 274.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  76%|██████████████▍    | 196/259 [53:44<3:04:01, 175.26s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:06:32] Done 60N_090W.tif in 270.98 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  76%|██████████████▍    | 197/259 [58:08<3:19:12, 192.78s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:10:57] Done 60N_100E.tif in 264.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  76%|████████████▉    | 198/259 [1:02:29<3:31:11, 207.74s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:15:18] Done 60N_100W.tif in 261.09 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  77%|█████████████    | 199/259 [1:07:01<3:43:06, 223.12s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:19:50] Done 60N_110E.tif in 272.28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  77%|█████████████▏   | 200/259 [1:11:18<3:47:49, 231.69s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:24:07] Done 60N_110W.tif in 256.88 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  78%|█████████████▏   | 201/259 [1:15:57<3:56:07, 244.27s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:28:46] Done 60N_120E.tif in 278.94 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  78%|█████████████▎   | 202/259 [1:20:24<3:57:56, 250.47s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:33:13] Done 60N_120W.tif in 266.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  78%|█████████████▎   | 203/259 [1:24:51<3:58:09, 255.18s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:37:40] Done 60N_130E.tif in 267.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  79%|█████████████▍   | 204/259 [1:29:20<3:57:23, 258.97s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:42:08] Done 60N_130W.tif in 268.36 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  79%|█████████████▍   | 205/259 [1:34:11<4:01:36, 268.46s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:47:00] Done 60N_140E.tif in 291.56 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  80%|█████████████▌   | 206/259 [1:38:16<3:51:03, 261.57s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:51:05] Done 60N_140W.tif in 245.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  80%|█████████████▌   | 207/259 [1:42:15<3:40:53, 254.87s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:55:04] Done 60N_150E.tif in 238.90 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  80%|█████████████▋   | 208/259 [1:46:26<3:35:37, 253.68s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 04:59:15] Done 60N_150W.tif in 250.84 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  81%|█████████████▋   | 209/259 [1:50:50<3:34:03, 256.87s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 05:03:39] Done 60N_160E.tif in 264.37 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  81%|█████████████▊   | 210/259 [1:55:15<3:31:40, 259.18s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 05:08:04] Done 60N_160W.tif in 264.63 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  81%|█████████████▊   | 211/259 [1:59:19<3:23:41, 254.62s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 05:12:08] Done 60N_170E.tif in 243.91 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles:  82%|█████████████▉   | 212/259 [2:03:31<3:18:51, 253.87s/tile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-18 05:16:20] Done 60N_170W.tif in 252.11 s\n"
     ]
    }
   ],
   "source": [
    "# Forest_Depth_ED_Mapping.py\n",
    "# Local 3x3 neighbor mosaic + halo Euclidean forest depth (meters).\n",
    "# 1 = forest, 0 = non-forest. Output: float32 meters; non-forest set to 0.\n",
    "#\n",
    "# How to run:\n",
    "#   python Forest_Depth_ED_Mapping.py\n",
    "#\n",
    "# Dependencies:\n",
    "#   pip install rasterio numpy scipy tqdm pyproj\n",
    "# (pyproj is optional but improves degree->meter accuracy.)\n",
    "#\n",
    "# Notes:\n",
    "# - Works whether rasters are in a projected CRS (meters) or in degrees (EPSG:4326 etc.).\n",
    "# - For degree CRS, meters-per-pixel is computed using local latitude (pyproj.Geod if available).\n",
    "# - Uses only the target tile + its 3×3 neighborhood and a pixel halo to avoid global VRTs.\n",
    "\n",
    "import os, math, time, warnings, gc\n",
    "from datetime import datetime\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import array_bounds\n",
    "from affine import Affine\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- USER SETTINGS (edit these paths if needed) ---------\n",
    "INPUT_DIR  = r\"/mnt/cephfs-mount/hangkai/2020\"\n",
    "OUTPUT_DIR = r\"/mnt/cephfs-mount/hangkai/2020_depth\"\n",
    "\n",
    "HALO_PX = 2048                # Pixel halo around each tile (e.g., 256 / 512 / 1024)\n",
    "SUBTRACT_HALF_PIXEL = True    # Better approximates distance to boundary vs pixel center\n",
    "OVERWRITE = False             # Skip tiles that already have outputs when False\n",
    "LIMIT_TILES = None            # e.g., 5 to test only first 5 tiles; set None for all\n",
    "COMPRESSION = \"LZW\"           # \"LZW\" or \"DEFLATE\" etc.\n",
    "\n",
    "# --------- Optional: accurate meters-per-degree via pyproj ---------\n",
    "try:\n",
    "    from pyproj import Geod\n",
    "    GEOD = Geod(ellps=\"WGS84\")\n",
    "except Exception:\n",
    "    GEOD = None\n",
    "\n",
    "\n",
    "# ====================== Utilities ======================\n",
    "def now_str():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def list_tifs(input_dir):\n",
    "    tifs = [os.path.join(input_dir, n) for n in os.listdir(input_dir)\n",
    "            if n.lower().endswith((\".tif\", \".tiff\"))]\n",
    "    tifs.sort()\n",
    "    return tifs\n",
    "\n",
    "def build_tile_index(tif_paths):\n",
    "    \"\"\"Gather per-tile metadata used for neighbor search and merging.\"\"\"\n",
    "    index = []\n",
    "    crs0 = None\n",
    "    for p in tqdm(tif_paths, desc=\"Indexing tiles\", unit=\"tile\"):\n",
    "        with rasterio.open(p) as ds:\n",
    "            h, w = ds.height, ds.width\n",
    "            btm, lft, top, rgt = array_bounds(h, w, ds.transform)  # (ymin, xmin, ymax, xmax)\n",
    "            if abs(ds.transform.b) > 1e-9 or abs(ds.transform.d) > 1e-9:\n",
    "                print(f\"[{now_str()}] WARNING: {os.path.basename(p)} has rotation/shear in transform.\")\n",
    "            if crs0 is None:\n",
    "                crs0 = ds.crs\n",
    "            elif ds.crs != crs0:\n",
    "                print(f\"[{now_str()}] WARNING: CRS differs for {os.path.basename(p)}.\")\n",
    "            index.append({\n",
    "                \"path\": p,\n",
    "                \"name\": os.path.basename(p),\n",
    "                \"width\": w,\n",
    "                \"height\": h,\n",
    "                \"transform\": ds.transform,\n",
    "                \"bounds\": (lft, btm, rgt, top),             # (L, B, R, T)\n",
    "                \"center\": ((lft+rgt)/2.0, (btm+top)/2.0),   # (cx, cy)\n",
    "                \"xres\": abs(ds.transform.a),\n",
    "                \"yres\": abs(ds.transform.e),\n",
    "                \"crs\": ds.crs\n",
    "            })\n",
    "    # Quick lookup by path\n",
    "    path2meta = {m[\"path\"]: m for m in index}\n",
    "    return index, path2meta\n",
    "\n",
    "def intersects(b1, b2):\n",
    "    L1,B1,R1,T1 = b1\n",
    "    L2,B2,R2,T2 = b2\n",
    "    return (L1 < R2) and (R1 > L2) and (B1 < T2) and (T1 > B2)\n",
    "\n",
    "def meters_per_pixel_from_shape(transform, width, height, crs, center_lat=None):\n",
    "    \"\"\"\n",
    "    Return (yres_m, xres_m).\n",
    "    If projected (meter) CRS -> return pixel sizes directly (with a crude feet->meters guard).\n",
    "    If geographic -> convert degree sizes to meters at provided/derived latitude.\n",
    "    \"\"\"\n",
    "    xres = abs(transform.a)\n",
    "    yres = abs(transform.e)\n",
    "\n",
    "    if crs and crs.is_projected:\n",
    "        # crude sniff for feet; avoids ~3.28x inflation if any tile is in US feet\n",
    "        try:\n",
    "            wkt = crs.to_wkt().lower()\n",
    "            if \"foot\" in wkt or \"feet\" in wkt or \"us-ft\" in wkt:\n",
    "                return (yres * 0.3048, xres * 0.3048)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return (yres, xres)\n",
    "\n",
    "    # Geographic: need representative latitude\n",
    "    if center_lat is None:\n",
    "        btm, lft, top, rgt = array_bounds(height, width, transform)\n",
    "        center_lat = (top + btm) / 2.0\n",
    "\n",
    "    if GEOD is not None:\n",
    "        # Use great-circle distances for 1 px steps\n",
    "        btm, lft, top, rgt = array_bounds(height, width, transform)\n",
    "        center_lon = (lft + rgt) / 2.0\n",
    "        _, _, dx_m = GEOD.inv(center_lon, center_lat, center_lon + xres, center_lat)\n",
    "        _, _, dy_m = GEOD.inv(center_lon, center_lat, center_lon, center_lat + yres)\n",
    "        return (abs(dy_m), abs(dx_m))\n",
    "\n",
    "    # Fallback approximation formulas\n",
    "    phi = math.radians(center_lat)\n",
    "    m_per_deg_lat = 111132.92 - 559.82*math.cos(2*phi) + 1.175*math.cos(4*phi) - 0.0023*math.cos(6*phi)\n",
    "    m_per_deg_lon = 111412.84*math.cos(phi) - 93.5*math.cos(3*phi) + 0.118*math.cos(5*phi)\n",
    "    return (yres * m_per_deg_lat, xres * m_per_deg_lon)\n",
    "\n",
    "\n",
    "def process_tile_local3x3(tile_meta, all_meta, halo_px, out_path,\n",
    "                          subtract_half_pixel=True, compress=\"LZW\"):\n",
    "    \"\"\"\n",
    "    Forest depth for one tile using a 3×3 neighborhood with integer-grid placement (no resampling).\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    - All tiles share CRS, pixel size, and are north-up (no rotation/shear).\n",
    "    - Tiles are perfectly aligned on the same grid.\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # ---- Center tile basics ----\n",
    "    W, H = tile_meta[\"width\"], tile_meta[\"height\"]\n",
    "    tx, crs = tile_meta[\"transform\"], tile_meta[\"crs\"]\n",
    "\n",
    "    # Safety: north-up only\n",
    "    if abs(tx.b) > 1e-9 or abs(tx.d) > 1e-9:\n",
    "        raise RuntimeError(\"Center tile has rotation/shear; integer-grid placement requires north-up.\")\n",
    "\n",
    "    a = tx.a                   # col scale (>0)\n",
    "    e = tx.e                   # row scale (<0 for north-up)\n",
    "    xres = abs(a)\n",
    "    yres = abs(e)\n",
    "\n",
    "    # ---- Target grid (expanded outward by HALO_PX) ----\n",
    "    DST_W  = W + 2*halo_px\n",
    "    DST_H  = H + 2*halo_px\n",
    "    dst_transform = tx * Affine.translation(-halo_px, -halo_px)\n",
    "\n",
    "    # Target mask: 0 = non-forest, 1 = forest\n",
    "    mosaic_u8 = np.zeros((DST_H, DST_W), dtype=np.uint8)\n",
    "\n",
    "    # ---- Load center tile once (for final mask + profile; short-circuit if empty) ----\n",
    "    with rasterio.open(tile_meta[\"path\"]) as src_center:\n",
    "        center_profile = src_center.profile\n",
    "        center_arr = src_center.read(1)\n",
    "\n",
    "    center_has_forest = bool((center_arr == 1).any())\n",
    "    if not center_has_forest:\n",
    "        # Nothing to compute—write zeros and return\n",
    "        core = np.zeros((H, W), dtype=np.float32)\n",
    "        center_profile.update(\n",
    "            driver=\"GTiff\", dtype=\"float32\", count=1, nodata=0.0,\n",
    "            compress=compress, predictor=3, tiled=True, blockxsize=512, blockysize=512,\n",
    "            bigtiff=\"IF_SAFER\", interleave=\"band\"\n",
    "        )\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        with rasterio.open(out_path, \"w\", **center_profile) as dst:\n",
    "            dst.write(core, 1)\n",
    "        del center_arr, core\n",
    "        gc.collect()\n",
    "        return time.perf_counter() - t0\n",
    "\n",
    "    # ---- Map neighbors to grid offsets using upper-left coordinates ----\n",
    "    cx0, cy0 = tx.c, tx.f  # UL of center tile\n",
    "\n",
    "    grid = {}\n",
    "    for m in all_meta:\n",
    "        mt = m[\"transform\"]\n",
    "        # Sanity checks\n",
    "        if m[\"crs\"] != crs: continue\n",
    "        if abs(mt.b) > 1e-9 or abs(mt.d) > 1e-9: continue  # rotated -> skip\n",
    "        if abs(mt.a - a) > 1e-9 or abs(mt.e - e) > 1e-9: continue  # different pixel size\n",
    "\n",
    "        nx0, ny0 = mt.c, mt.f  # neighbor UL world coords\n",
    "        dc = (nx0 - cx0) / a   # columns offset (float)\n",
    "        dr = (ny0 - cy0) / e   # rows offset (float)  (e<0)\n",
    "\n",
    "        dx_step = int(round(dc / W))\n",
    "        dy_step = int(round(dr / H))\n",
    "        if dx_step < -1 or dx_step > 1 or dy_step < -1 or dy_step > 1:\n",
    "            continue\n",
    "\n",
    "        key = (dy_step, dx_step)\n",
    "        if key not in grid:\n",
    "            grid[key] = m\n",
    "\n",
    "    # Ensure center tile itself exists\n",
    "    grid.setdefault((0, 0), tile_meta)\n",
    "\n",
    "    # ---- Paste each neighbor onto the target canvas (integer indices only) ----\n",
    "    def paste_neighbor_exact(src_path, dy_step, dx_step):\n",
    "        \"\"\"\n",
    "        Place one neighbor on the target canvas by integer offsets.\n",
    "        \"\"\"\n",
    "        # Destination top-left for this neighbor in the mosaic (in pixels)\n",
    "        dr0 = halo_px + dy_step * H    # rows\n",
    "        dc0 = halo_px + dx_step * W    # cols\n",
    "\n",
    "        # If completely outside the canvas, skip\n",
    "        if dr0 >= DST_H or dc0 >= DST_W or dr0 + H <= 0 or dc0 + W <= 0:\n",
    "            return\n",
    "\n",
    "        # Source start offsets if the neighbor starts outside the canvas\n",
    "        sr0 = max(0, -dr0)             # rows to skip at source top\n",
    "        sc0 = max(0, -dc0)             # cols to skip at source left\n",
    "        dr0c = max(0, dr0)             # clamped dest row\n",
    "        dc0c = max(0, dc0)             # clamped dest col\n",
    "\n",
    "        # Intended size\n",
    "        hh = min(H - sr0, DST_H - dr0c)   # rows (height)\n",
    "        ww = min(W - sc0, DST_W - dc0c)   # cols  (width)\n",
    "        if hh <= 0 or ww <= 0:\n",
    "            return\n",
    "\n",
    "        with rasterio.open(src_path, sharing=False) as src:\n",
    "            # FIXED: Window order is (col_off, row_off, width, height)\n",
    "            patch = src.read(1, window=Window(sc0, sr0, ww, hh))\n",
    "\n",
    "        # Strict 0/1 forest mask\n",
    "        patch = (patch == 1).astype(np.uint8)\n",
    "\n",
    "        # Destination view\n",
    "        view = mosaic_u8[dr0c:dr0c+hh, dc0c:dc0c+ww]\n",
    "\n",
    "        # Shapes must match; clip if necessary (should rarely be needed)\n",
    "        if patch.shape != view.shape:\n",
    "            h_eff = min(patch.shape[0], view.shape[0])\n",
    "            w_eff = min(patch.shape[1], view.shape[1])\n",
    "            patch = patch[:h_eff, :w_eff]\n",
    "            view  = mosaic_u8[dr0c:dr0c+h_eff, dc0c:dc0c+w_eff]\n",
    "\n",
    "        np.maximum(view, patch, out=view)\n",
    "\n",
    "    # Paste center first, then neighbors so center wins on overlaps\n",
    "    order = [(0,0), (-1,0),(+1,0),(0,-1),(0,+1),(-1,-1),(-1,+1),(+1,-1),(+1,+1)]\n",
    "    for dy_step, dx_step in order:\n",
    "        m = grid.get((dy_step, dx_step))\n",
    "        if not m:\n",
    "            continue\n",
    "        paste_neighbor_exact(m[\"path\"], dy_step, dx_step)\n",
    "\n",
    "    # ---- EDT on merged mask ----\n",
    "    # Pixel spacing (meters) from the target grid\n",
    "    ymin, xmin, ymax, xmax = array_bounds(DST_H, DST_W, dst_transform)\n",
    "    center_lat = (ymax + ymin) / 2.0\n",
    "    yres_m, xres_m = meters_per_pixel_from_shape(dst_transform, DST_W, DST_H, crs, center_lat)\n",
    "\n",
    "    forest_mask = (mosaic_u8 == 1)\n",
    "    del mosaic_u8; gc.collect()\n",
    "\n",
    "    # GUARD: if no background (all forest in 3×3 + halo), write zeros and exit\n",
    "    if not np.any(~forest_mask):\n",
    "        core = np.zeros((H, W), dtype=np.float32)\n",
    "        profile = center_profile\n",
    "        profile.update(\n",
    "            driver=\"GTiff\", dtype=\"float32\", count=1, nodata=0.0,\n",
    "            compress=compress, predictor=3, tiled=True, blockxsize=512, blockysize=512,\n",
    "            bigtiff=\"IF_SAFER\", interleave=\"band\"\n",
    "        )\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "            dst.write(core, 1)\n",
    "        del forest_mask, core\n",
    "        gc.collect()\n",
    "        return time.perf_counter() - t0\n",
    "\n",
    "    # Distance transform with anisotropic sampling (meters)\n",
    "    dt64 = distance_transform_edt(forest_mask, sampling=(yres_m, xres_m))\n",
    "    dist_chunk = dt64.astype(np.float32)\n",
    "    del dt64, forest_mask; gc.collect()\n",
    "\n",
    "    if subtract_half_pixel:\n",
    "        # Boundary (not center-to-center) distance\n",
    "        half_min = 0.5 * min(yres_m, xres_m)\n",
    "        np.maximum(dist_chunk - half_min, 0.0, out=dist_chunk)\n",
    "\n",
    "    # ---- Crop the center tile core (exactly H×W at (halo,halo)) ----\n",
    "    core = dist_chunk[HALO_PX:HALO_PX+H, HALO_PX:HALO_PX+W]\n",
    "    assert core.shape == (H, W)\n",
    "    del dist_chunk; gc.collect()\n",
    "\n",
    "    # ---- Final mask: keep values only where center tile itself is forest ----\n",
    "    core = np.where(center_arr == 1, core, 0.0).astype(np.float32)\n",
    "    del center_arr; gc.collect()\n",
    "\n",
    "    # ---- Sanitize: force NaN/Inf/nodata to 0, clamp to [0, MAX] (optional clamp) ----\n",
    "    MAX_VAL = 1_000_000.0\n",
    "    core = np.nan_to_num(core, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "    np.clip(core, 0.0, MAX_VAL, out=core)\n",
    "\n",
    "    # ---- Write single-band float32 GTiff (LZW + predictor=3) with nodata=0 ----\n",
    "    profile = center_profile\n",
    "    profile.update(\n",
    "        driver=\"GTiff\", dtype=\"float32\", count=1, nodata=0.0,\n",
    "        compress=compress, predictor=3, tiled=True, blockxsize=512, blockysize=512,\n",
    "        bigtiff=\"IF_SAFER\", interleave=\"band\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        dst.write(core, 1)\n",
    "\n",
    "    del core\n",
    "    gc.collect()\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "\n",
    "# ====================== Main Routine ======================\n",
    "def main():\n",
    "    os.environ.setdefault(\"GDAL_PAM_ENABLED\", \"NO\")  # no .aux.xml\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    tif_list = list_tifs(INPUT_DIR)\n",
    "    if not tif_list:\n",
    "        print(f\"[{now_str()}] No GeoTIFFs found in: {INPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    if LIMIT_TILES is not None:\n",
    "        tif_list = tif_list[:int(LIMIT_TILES)]\n",
    "\n",
    "    print(f\"[{now_str()}] Mode: LOCAL 3x3 + HALO\")\n",
    "    print(f\"  Input : {INPUT_DIR}\")\n",
    "    print(f\"  Output: {OUTPUT_DIR}\")\n",
    "    print(f\"  Files : {len(tif_list)}\")\n",
    "    print(f\"  Halo (px): {HALO_PX}\")\n",
    "    print(f\"  Subtract half pixel: {SUBTRACT_HALF_PIXEL}\\n\")\n",
    "\n",
    "    # Build metadata index once\n",
    "    index, path2meta = build_tile_index(tif_list)\n",
    "\n",
    "    total_start = time.perf_counter()\n",
    "    failures = 0\n",
    "\n",
    "    for p in tqdm(tif_list, desc=\"Processing tiles\", unit=\"tile\"):\n",
    "        name = os.path.basename(p)\n",
    "        out_name = os.path.splitext(name)[0] + \"_depth_m.tif\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "\n",
    "        if (not OVERWRITE) and os.path.exists(out_path):\n",
    "            tqdm.write(f\"[{now_str()}] Skip (exists): {out_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tile_meta = path2meta[p]\n",
    "            dt = process_tile_local3x3(\n",
    "                tile_meta, index, HALO_PX, out_path,\n",
    "                subtract_half_pixel=SUBTRACT_HALF_PIXEL,\n",
    "                compress=COMPRESSION\n",
    "            )\n",
    "            tqdm.write(f\"[{now_str()}] Done {name} in {dt:.2f} s\")\n",
    "        except Exception as e:\n",
    "            failures += 1\n",
    "            tqdm.write(f\"[{now_str()}] ERROR {name}: {e}\")\n",
    "\n",
    "    total_s = time.perf_counter() - total_start\n",
    "    print(f\"\\n[{now_str()}] All done in {total_s/60.0:.2f} min. Failures: {failures}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d9552c-a2dd-4169-8b32-78c09c2b04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47 zip files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:   0%|                                                                                | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipping ERA5Land_2018_08 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2018_09 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2018_10 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2018_11 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2018_12 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_01 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_02 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_03 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_04 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_05 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_06 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_07 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_08 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_09 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2019_10 (already unzipped)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files: 100%|███████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipping ERA5Land_2021_01 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_02 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_03 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_04 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_05 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_06 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_07 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_08 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_09 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_10 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_11 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2021_12 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_01 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_02 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_03 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_04 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_05 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_06 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_07 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_08 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_09 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_10 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_11 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2022_12 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2023_01 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2023_02 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2025_01 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2025_02 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2025_03 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2025_04 (already unzipped)\n",
      "⏩ Skipping ERA5Land_2025_05 (already unzipped)\n",
      "✅ All files processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = Path(\"/mnt/cephfs-mount/chenchen/ERA5_Climate_Data\")\n",
    "output_base = base_dir / \"unzip\"\n",
    "output_base.mkdir(exist_ok=True)\n",
    "\n",
    "# === Find all .zip files ===\n",
    "zip_files = sorted(base_dir.glob(\"*.zip\"))\n",
    "\n",
    "print(f\"Found {len(zip_files)} zip files.\")\n",
    "\n",
    "# === Unzip loop ===\n",
    "for zip_path in tqdm(zip_files, desc=\"Unzipping files\"):\n",
    "    # e.g. ERA5Land_2018_08_daily_mean.zip → ERA5Land_2018_08\n",
    "    subfolder_name = \"_\".join(zip_path.stem.split(\"_\")[:3])\n",
    "    output_folder = output_base / subfolder_name\n",
    "\n",
    "    # Skip if already unzipped\n",
    "    if output_folder.exists() and any(output_folder.iterdir()):\n",
    "        print(f\"⏩ Skipping {subfolder_name} (already unzipped)\")\n",
    "        continue\n",
    "\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Unzip content\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(output_folder)\n",
    "\n",
    "print(\"✅ All files processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20adf3c-588b-49a3-9f16-d654be92c420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from geopy.distance import geodesic\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate lookup table for pixel widths\n",
    "def compute_width_lookup(num_rows, transform):\n",
    "    latitudes = [transform[5] + i * transform[4] for i in range(num_rows)]\n",
    "    return [compute_width(lat, abs(transform[0])) for lat in latitudes]\n",
    "\n",
    "# Function to calculate pixel width\n",
    "def compute_width(lat, cellsize):\n",
    "    point1 = (lat, 0)\n",
    "    point2 = (lat, cellsize)\n",
    "    return geodesic(point1, point2).meters\n",
    "\n",
    "# Function to calculate pixel height\n",
    "def compute_height(cellsize):\n",
    "    point1 = (0, 0)\n",
    "    point2 = (0, cellsize)\n",
    "    return geodesic(point1, point2).meters\n",
    "\n",
    "# Function to compute forest edge length\n",
    "def compute_edge_length(i, j, forest, width_lookup, height):\n",
    "    edge_length = 0.0  # Initialize edge length\n",
    "\n",
    "    # Check left pixel\n",
    "    if j > 0 and not forest[i, j-1]:\n",
    "        edge_length += height  # Add height to edge length\n",
    "\n",
    "    # Check right pixel\n",
    "    if j < forest.shape[1] - 1 and not forest[i, j+1]:\n",
    "        edge_length += height  # Add height to edge length\n",
    "\n",
    "    # Check upper pixel\n",
    "    if i > 0 and not forest[i-1, j]:\n",
    "        edge_length += width_lookup[i-1]  # Add width of upper pixel to edge length\n",
    "\n",
    "    # Check lower pixel\n",
    "    if i < forest.shape[0] - 1 and not forest[i+1, j]:\n",
    "        edge_length += width_lookup[i]  # Add width of current pixel to edge length\n",
    "\n",
    "    return edge_length\n",
    "\n",
    "# Function to process a single TIFF file\n",
    "def process_tiff(filename):\n",
    "    # Full paths to input and output files\n",
    "    input_file = os.path.join(input_folder, filename)\n",
    "    output_file_area = os.path.join(output_folder_area, filename)\n",
    "    output_file_edge = os.path.join(output_folder_edge, filename)\n",
    "    #print(f\"Processing file: {filename}\")\n",
    "\n",
    "    # Open tif file\n",
    "    with rasterio.open(input_file) as ds:\n",
    "        forest = ds.read(1).astype(bool)\n",
    "        transform = ds.transform\n",
    "        cellsize = abs(transform[0])\n",
    "        #print(f\"Cellsize: {cellsize}\")\n",
    "\n",
    "        # Compute lookup table for pixel widths and pixel height\n",
    "        width_lookup = compute_width_lookup(forest.shape[0], transform)\n",
    "        height = compute_height(cellsize)\n",
    "        #print(\"Computed lookup table for pixel widths and pixel height\")\n",
    "\n",
    "        # Create new tifs\n",
    "        area_tif = np.zeros_like(forest, dtype=np.float32)\n",
    "        edge_tif = np.zeros_like(forest, dtype=np.float32)\n",
    "        #print(\"Created new tifs\")\n",
    "\n",
    "        # Loop through each pixel in the forest tif\n",
    "        for i in range(forest.shape[0]):\n",
    "            for j in range(forest.shape[1]):\n",
    "                # Check if the pixel is a forest pixel\n",
    "                if forest[i, j]:\n",
    "                    # Calculate the area of the pixel and assign it to area_tif\n",
    "                    area_tif[i, j] = width_lookup[i] * height\n",
    "\n",
    "                    # Calculate the edge length of the pixel and assign it to edge_tif\n",
    "                    edge_tif[i, j] = compute_edge_length(i, j, forest, width_lookup, height)\n",
    "\n",
    "        # Convert to unsigned integers with rounding\n",
    "        scale_factor = 10\n",
    "        area_tif = np.around(area_tif * scale_factor).astype(np.uint16)\n",
    "        edge_tif = np.around(edge_tif * scale_factor).astype(np.uint16)\n",
    "\n",
    "        # Write new tif to file\n",
    "        with rasterio.open(output_file_area, 'w', driver='GTiff', height=area_tif.shape[0],\n",
    "                           width=area_tif.shape[1], count=1, compress='lzw', dtype=area_tif.dtype,\n",
    "                           crs=ds.crs, transform=transform) as dst:\n",
    "            dst.write(area_tif, 1)\n",
    "\n",
    "        with rasterio.open(output_file_edge, 'w', driver='GTiff', height=edge_tif.shape[0],\n",
    "                           width=edge_tif.shape[1], count=1, compress='lzw', dtype=edge_tif.dtype,\n",
    "                           crs=ds.crs, transform=transform) as dst:\n",
    "            dst.write(edge_tif, 1)\n",
    "\n",
    "    print(f\"Finished processing file: {filename}\")\n",
    "    del forest\n",
    "    del area_tif\n",
    "    del edge_tif\n",
    "    gc.collect()  # Force garbage collection\n",
    "\n",
    "# Paths to input and output folders\n",
    "input_folder = \"/mnt/cephfs-mount/hangkai/2015/\"\n",
    "output_folder_area = \"/mnt/cephfs-mount/hangkai/2015Area\"\n",
    "output_folder_edge = \"/mnt/cephfs-mount/hangkai/2015Edge\"\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "if not os.path.exists(output_folder_area):\n",
    "    os.makedirs(output_folder_area)\n",
    "if not os.path.exists(output_folder_edge):\n",
    "    os.makedirs(output_folder_edge)\n",
    "\n",
    "# Get list of tif files\n",
    "tif_files = [f for f in os.listdir(input_folder) if f.endswith(\".tif\")]\n",
    "print(\"Start to loop!\")\n",
    "# Loop over each file in the list\n",
    "for filename in tqdm(tif_files):\n",
    "    process_tiff(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917834e-6fa6-46b9-8f05-a69a9187049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "from geopy.distance import geodesic\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "################################################################################################################################\n",
    "def get_surrounding_files(file_name):\n",
    "    \"\"\"Given a file name, return the file names of the files corresponding to \n",
    "    the geographical locations to the north, south, east, and west.\"\"\"\n",
    "\n",
    "    def extract_first_digit(string):\n",
    "        \"\"\"Extract the first sequence of digits from the string.\"\"\"\n",
    "        pattern = r'\\d+'  # Match one or more numbers\n",
    "        match = re.search(pattern, string)\n",
    "        return int(match.group()) if match else None\n",
    "\n",
    "    def extract_second_digit(string):\n",
    "        \"\"\"Extract the second sequence of digits from the string.\"\"\"\n",
    "        pattern = r'\\d+'  # Match one or more numbers\n",
    "        matches = re.findall(pattern, string)\n",
    "        return int(matches[1]) if len(matches) >= 2 else None\n",
    "\n",
    "    def extract_first_letter(string):\n",
    "        \"\"\"Extract the first English letter in the string.\"\"\"\n",
    "        for char in string:\n",
    "            if char.isalpha(): return char\n",
    "\n",
    "    def extract_second_letter(string):\n",
    "        \"\"\"Extract the second English letter in the string.\"\"\"\n",
    "        count = 0\n",
    "        for char in string:\n",
    "            if char.isalpha(): \n",
    "                count += 1\n",
    "                if count == 2: return char\n",
    "\n",
    "    first_digit = extract_first_digit(file_name)\n",
    "    second_digit = extract_second_digit(file_name)\n",
    "    first_letter = extract_first_letter(file_name)\n",
    "    second_letter = extract_second_letter(file_name)\n",
    "\n",
    "    if first_letter == 'N':\n",
    "        up_first_digit = int(first_digit) + 10\n",
    "        up_first_letter = 'N'\n",
    "        if first_digit == 0:\n",
    "            up_first_digit = 10\n",
    "            up_first_letter = 'N'\n",
    "            down_first_digit = 10\n",
    "            down_first_letter = 'S'\n",
    "            up_second_digit = second_digit\n",
    "            down_second_digit = second_digit\n",
    "            up_second_letter = second_letter\n",
    "            down_second_letter = second_letter\n",
    "        else:\n",
    "            up_first_digit = first_digit + 10\n",
    "            down_first_digit = int(first_digit) - 10\n",
    "            down_first_letter = 'N'\n",
    "            up_first_letter = 'N'\n",
    "            up_second_digit = second_digit\n",
    "            down_second_digit = second_digit\n",
    "            up_second_letter = second_letter\n",
    "            down_second_letter = second_letter\n",
    "    elif first_letter == 'S':\n",
    "        if first_digit == 10:\n",
    "            up_first_digit = 0\n",
    "            up_first_letter = 'N'\n",
    "            down_first_digit = 20\n",
    "            down_first_letter = 'S'\n",
    "            up_second_digit = second_digit\n",
    "            down_second_digit = second_digit\n",
    "            up_second_letter = second_letter\n",
    "            down_second_letter = second_letter\n",
    "        else:\n",
    "            up_first_digit = int(first_digit) - 10\n",
    "            up_first_letter = 'S'\n",
    "            down_first_digit = int(first_digit) + 10\n",
    "            down_first_letter = 'S'\n",
    "            up_second_digit = second_digit\n",
    "            down_second_digit = second_digit\n",
    "            up_second_letter = second_letter\n",
    "            down_second_letter = second_letter\n",
    "\n",
    "    # East or West\n",
    "    if second_letter == 'E':\n",
    "        right_second_digit = second_digit + 10\n",
    "        right_second_letter = second_letter\n",
    "        left_second_digit = second_digit - 10\n",
    "        left_second_letter = second_letter\n",
    "        if second_digit == 0:\n",
    "            right_second_digit = second_digit + 10\n",
    "            right_second_letter = second_letter\n",
    "            left_second_digit = 10\n",
    "            left_second_letter = 'W'\n",
    "        if second_digit == 170:\n",
    "            right_second_digit = 180\n",
    "            right_second_letter = 'W'\n",
    "            left_second_digit = second_digit - 10\n",
    "            left_second_letter = 'E'\n",
    "    elif second_letter == 'W':\n",
    "        right_second_digit = second_digit - 10\n",
    "        right_second_letter = second_letter\n",
    "        left_second_digit = second_digit + 10\n",
    "        left_second_letter = second_letter\n",
    "        if second_digit == 10:\n",
    "            right_second_digit = 0\n",
    "            right_second_letter = 'E'\n",
    "            left_second_digit = 20\n",
    "            left_second_letter = 'W'\n",
    "        if second_digit == 180:\n",
    "            right_second_digit = 170\n",
    "            right_second_letter = 'W'\n",
    "            left_second_digit = 170\n",
    "            left_second_letter = 'E'\n",
    "        \n",
    "    north_file_name = \"{:02d}{}_{:03d}{}.tif\".format(abs(up_first_digit), up_first_letter, abs(second_digit), second_letter)\n",
    "    south_file_name = \"{:02d}{}_{:03d}{}.tif\".format(abs(down_first_digit), down_first_letter, abs(second_digit), second_letter)\n",
    "    east_file_name = \"{:02d}{}_{:03d}{}.tif\".format(abs(first_digit), first_letter, abs(right_second_digit), right_second_letter)\n",
    "    west_file_name = \"{:02d}{}_{:03d}{}.tif\".format(abs(first_digit), first_letter, abs(left_second_digit), left_second_letter)\n",
    "\n",
    "    return north_file_name, south_file_name, east_file_name, west_file_name\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "def get_border_pixels(file_name):\n",
    "    with rasterio.open(file_name) as dataset:\n",
    "        band1 = dataset.read(1)  # assuming you want to read the first band\n",
    "\n",
    "        north_pixels = band1[0, :]  # North border pixels\n",
    "        south_pixels = band1[-1, :]  # South border pixels\n",
    "        east_pixels = band1[:, -1]  # East border pixels\n",
    "        west_pixels = band1[:, 0]  # West border pixels\n",
    "\n",
    "    return north_pixels, south_pixels, east_pixels, west_pixels\n",
    "\n",
    "def process_files(input_folder, central_file):\n",
    "    # Calculate file names of the surrounding files\n",
    "    north_file, south_file, east_file, west_file = get_surrounding_files(central_file)\n",
    "\n",
    "    # List of files and corresponding border pixels\n",
    "    file_list = [(north_file, 1), (south_file, 0), (east_file, 3), (west_file, 2)]\n",
    "\n",
    "    # Dictionary to store file existence flags and border pixels\n",
    "    border_data = {}\n",
    "\n",
    "    # Get the border pixels of the central file\n",
    "    central_file_path = os.path.join(input_folder, central_file)\n",
    "    if os.path.exists(central_file_path):\n",
    "        central_pixels = get_border_pixels(central_file_path)\n",
    "    else:\n",
    "        print(f\"Warning: {central_file} does not exist. Skipping this file.\")\n",
    "        return None\n",
    "\n",
    "    for file, border_index in file_list:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # For north and south files, we need to consider the south and north borders of the central file, respectively.\n",
    "            if border_index in [0, 1]:\n",
    "                central_border_pixels = central_pixels[1 - border_index]  \n",
    "            # For east and west files, we need to consider the west and east borders of the central file, respectively.\n",
    "            else:  \n",
    "                central_border_pixels = central_pixels[5 - border_index]\n",
    "            border_data[file] = {\n",
    "                \"exists\": True, \n",
    "                \"pixels\": get_border_pixels(file_path)[border_index], \n",
    "                \"central_pixels\": central_border_pixels\n",
    "            }\n",
    "        else:\n",
    "            border_data[file] = {\"exists\": False}\n",
    "    return border_data\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "def read_border_pixels(file_path):\n",
    "    dataset = rasterio.open(file_path)\n",
    "\n",
    "    width = dataset.width\n",
    "    height = dataset.height\n",
    "\n",
    "    upper_pixels = dataset.read(1, window=((0, 1), (0, width)))\n",
    "    lower_pixels = dataset.read(1, window=((height-1, height), (0, width)))\n",
    "    left_pixels = dataset.read(1, window=((0, height), (0, 1)))\n",
    "    right_pixels = dataset.read(1, window=((0, height), (width-1, width)))\n",
    "\n",
    "    left_pixels = np.reshape(left_pixels, (height,))\n",
    "    right_pixels = np.reshape(right_pixels, (height,))\n",
    "    upper_pixels = np.reshape(upper_pixels, (width,))\n",
    "    lower_pixels = np.reshape(lower_pixels, (width,))\n",
    "\n",
    "    return left_pixels, right_pixels, upper_pixels, lower_pixels\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "def read_border_pixels_edge(file_path):\n",
    "    dataset = rasterio.open(file_path)\n",
    "    width = dataset.width\n",
    "    height = dataset.height\n",
    "\n",
    "    upper_pixels = dataset.read(1, window=((0, 1), (0, width)))\n",
    "    lower_pixels = dataset.read(1, window=((height-1, height), (0, width)))\n",
    "    left_pixels = dataset.read(1, window=((0, height), (0, 1)))\n",
    "    right_pixels = dataset.read(1, window=((0, height), (width-1, width)))\n",
    "\n",
    "    left_pixels = np.reshape(left_pixels, (height,))\n",
    "    right_pixels = np.reshape(right_pixels, (height,))\n",
    "    upper_pixels = np.reshape(upper_pixels, (width,))\n",
    "    lower_pixels = np.reshape(lower_pixels, (width,))\n",
    "\n",
    "    return left_pixels, right_pixels, upper_pixels, lower_pixels\n",
    "\n",
    "################################################################################################################################\n",
    "# Function to calculate lookup table for pixel widths\n",
    "def compute_width_lookup(num_rows, transform):\n",
    "    latitudes = [transform[5] + i * transform[4] for i in range(num_rows)]\n",
    "    return [compute_width(lat, abs(transform[0])) for lat in latitudes]\n",
    "\n",
    "# Function to calculate pixel width\n",
    "def compute_width(lat, cellsize):\n",
    "    point1 = (lat, 0)\n",
    "    point2 = (lat, cellsize)\n",
    "    return geodesic(point1, point2).meters\n",
    "\n",
    "# Function to calculate pixel height\n",
    "def compute_height(cellsize):\n",
    "    point1 = (0, 0)\n",
    "    point2 = (0, cellsize)\n",
    "    return geodesic(point1, point2).meters\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "# Paths for your folders\n",
    "input_folder = \"/mnt/cephfs-mount/hangkai/2015\"\n",
    "output_folder_edge = \"/mnt/cephfs-mount/hangkai/2015Edge\"\n",
    "input_folder_edge = output_folder_edge\n",
    "# Get all files in each folder\n",
    "central_files = os.listdir(input_folder)\n",
    "central_files_edge = os.listdir(output_folder_edge)\n",
    "\n",
    "# Ensuring that the files in both folders correspond to each other\n",
    "assert set(central_files) == set(central_files_edge)\n",
    "\n",
    "print('All functions loaded! Start to correct all the tifs.')\n",
    "\n",
    "# Loop through each file\n",
    "for central_file in tqdm(central_files):\n",
    "    # Ensure the file is a .tif file\n",
    "    if central_file.endswith('.tif'):\n",
    "        #print(f\"Processing file: {central_file}\")\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "        # Your processing code\n",
    "        # replace 'process_files', 'read_border_pixels' and 'read_border_pixels_edge' with your actual functions\n",
    "        # Usage:\n",
    "\n",
    "        border_data = process_files(input_folder, central_file)\n",
    "        north_file, south_file, east_file, west_file = get_surrounding_files(central_file)\n",
    "        central_file_path = os.path.join(input_folder, central_file)\n",
    "        left_pixels, right_pixels, upper_pixels, lower_pixels = read_border_pixels(central_file_path)\n",
    "\n",
    "        central_file_path_edge = os.path.join(input_folder_edge, central_file)\n",
    "        left_pixels_edge, right_pixels_edge, upper_pixels_edge, lower_pixels_edge = read_border_pixels_edge(central_file_path_edge)\n",
    "        left_pixels_edge_new = copy.copy(left_pixels_edge)\n",
    "        right_pixels_edge_new = copy.copy(right_pixels_edge)\n",
    "        upper_pixels_edge_new = copy.copy(upper_pixels_edge)\n",
    "        lower_pixels_edge_new = copy.copy(lower_pixels_edge)\n",
    "\n",
    "        # Calculate the width of the top and bottom row of pixels\n",
    "        central_file_path_edge = os.path.join(input_folder_edge, central_file)\n",
    "        dataset = rasterio.open(central_file_path)\n",
    "        width = dataset.width\n",
    "        height = dataset.height\n",
    "        width_lookup = compute_width_lookup(dataset.height, dataset.transform)\n",
    "        top_row_width = width_lookup[0]\n",
    "        bottom_row_width = width_lookup[-1]\n",
    "\n",
    "        # Calculate the height of the left and right columns of pixels\n",
    "        left_column_height = compute_height(abs(dataset.transform[4]))\n",
    "        right_column_height = compute_height(abs(dataset.transform[0]))\n",
    "\n",
    "\n",
    "        if border_data[north_file]['exists']:\n",
    "            for i in range(len(border_data[north_file]['pixels'])):\n",
    "                b = upper_pixels[i]\n",
    "                if b == 0:\n",
    "                    continue\n",
    "                a = (border_data[north_file]['pixels'][i])\n",
    "                if a*b == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    upper_pixels_edge_new[i] = upper_pixels_edge[i] + top_row_width\n",
    "        if border_data[south_file]['exists']:\n",
    "            for i in range(len(border_data[south_file]['pixels'])):\n",
    "                b = lower_pixels[i]\n",
    "                if b == 0:\n",
    "                    continue\n",
    "                a = (border_data[south_file]['pixels'][i])\n",
    "                if a*b == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    lower_pixels_edge_new[i] = lower_pixels_edge[i] + bottom_row_width\n",
    "        if border_data[west_file]['exists']:\n",
    "            for i in range(len(border_data[west_file]['pixels'])):\n",
    "                b = left_pixels[i] \n",
    "                if b == 0:\n",
    "                    continue\n",
    "                a = (border_data[west_file]['pixels'][i])\n",
    "                if a*b == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    left_pixels_edge_new[i] = left_pixels_edge[i] + left_column_height\n",
    "        if border_data[east_file]['exists']:\n",
    "            for i in range(len(border_data[east_file]['pixels'])):\n",
    "                b = right_pixels[i]\n",
    "                if b == 0:\n",
    "                    continue\n",
    "                a = (border_data[east_file]['pixels'][i])\n",
    "                if a*b == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    right_pixels_edge_new[i] = right_pixels_edge[i] + right_column_height\n",
    "\n",
    "        with rasterio.open(central_file_path_edge, 'r+') as src:\n",
    "            # Read the data\n",
    "            data = src.read(1)\n",
    "\n",
    "            # Replace the border\n",
    "            data[0, :] = upper_pixels_edge_new* 10\n",
    "            data[-1, :] = lower_pixels_edge_new* 10\n",
    "            data[:, 0] = left_pixels_edge_new* 10\n",
    "            data[:, -1] = right_pixels_edge_new* 10\n",
    "\n",
    "            # Convert the data to np.uint16\n",
    "            data = data.astype(np.uint16)\n",
    "\n",
    "            # Write the data\n",
    "            src.write(data, 1)\n",
    "            del data\n",
    "            gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
