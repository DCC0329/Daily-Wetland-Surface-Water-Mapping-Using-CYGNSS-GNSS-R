{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4504e669-83c4-41be-9132-93464c009e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9da15b5-cb08-438e-bcc9-5e29c3d6ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= User settings =========\n",
    "input_root = \"/mnt/cephfs-mount/chenchen/CygnssData/\"   # monthly subfolders like 2018_08 ... 2024_04 CHANGE YOUR OWN PATH HERE!!!\n",
    "output_root = \"/mnt/cephfs-mount/chenchen/CygnssDataCsv\"  # output root\n",
    "years_to_process = [2025]  # e.g., [2023, 2024] or None for all years CHANGE YOUR OWN PATH HERE!!!\n",
    " \n",
    "# ROI (fast approx): major/minor axes (km)\n",
    "ROI_MAJOR_KM = 3.5\n",
    "ROI_MINOR_KM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed263104-ba07-4ddc-956e-bc0ea062378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Variables expected in each NetCDF =========\n",
    "# sp_theta_orbit is optional; we handle its absence gracefully\n",
    "required_vars = [\n",
    "    \"sample\", \"ddm\", \"sp_lon\", \"sp_lat\", \"ddm_snr\",\n",
    "    \"gps_tx_power_db_w\", \"tx_to_sp_range\", \"rx_to_sp_range\",\n",
    "    \"sp_inc_angle\", \"gps_ant_gain_db_i\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df64c549-4fe0-4848-97ff-4d81089cc6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202501 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202501: per-day: 100%|████████████████████████| 31/31 [1:51:45<00:00, 216.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202502 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202502: per-day: 100%|████████████████████████| 28/28 [1:37:50<00:00, 209.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202503 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202503: per-day: 100%|████████████████████████| 31/31 [1:48:48<00:00, 210.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202504 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202504: per-day: 100%|████████████████████████| 30/30 [1:43:55<00:00, 207.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202505 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202505: per-day: 100%|████████████████████████| 31/31 [1:47:01<00:00, 207.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202506 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202506: per-day: 100%|████████████████████████| 30/30 [1:44:27<00:00, 208.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month: 202507 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202507: per-day: 100%|████████████████████████| 31/31 [1:50:24<00:00, 213.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DONE] All requested years processed.\n"
     ]
    }
   ],
   "source": [
    "# ========= Helpers =========\n",
    "import os, re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_month_folder_name(name):\n",
    "    # Accept 201808 / 2018_08 / 2018-08\n",
    "    m = re.match(r'^(\\d{4})[-_]?(\\d{2})$', name)\n",
    "    if not m: return None, None\n",
    "    y, mm = int(m.group(1)), int(m.group(2))\n",
    "    return (y, mm) if 1 <= mm <= 12 else (None, None)\n",
    "\n",
    "def normalize_years(years_to_process):\n",
    "    return None if years_to_process is None else {int(y) for y in years_to_process}\n",
    "\n",
    "def find_month_dirs(input_root, years_to_process=None):\n",
    "    \"\"\"Return list of tuples: (name, year, month, full_path)\"\"\"\n",
    "    yfilter = normalize_years(years_to_process)\n",
    "    out = []\n",
    "\n",
    "    # Case A: month folders directly under input_root (e.g., 201808)\n",
    "    for name in sorted(os.listdir(input_root)):\n",
    "        p = os.path.join(input_root, name)\n",
    "        if not os.path.isdir(p): \n",
    "            continue\n",
    "        y, m = parse_month_folder_name(name)\n",
    "        if y is not None and (yfilter is None or y in yfilter):\n",
    "            out.append((name, y, m, p))\n",
    "\n",
    "    if out:\n",
    "        return out  # already found direct month folders\n",
    "\n",
    "    # Case B: year subfolders (e.g., input_root/2018/201808 or /08)\n",
    "    for yname in sorted(os.listdir(input_root)):\n",
    "        ypath = os.path.join(input_root, yname)\n",
    "        if not os.path.isdir(ypath): \n",
    "            continue\n",
    "        # year folder like 2018\n",
    "        my = re.match(r'^(\\d{4})$', yname)\n",
    "        if not my: \n",
    "            continue\n",
    "        y = int(my.group(1))\n",
    "        if yfilter is not None and y not in yfilter:\n",
    "            continue\n",
    "\n",
    "        for mname in sorted(os.listdir(ypath)):\n",
    "            mpath = os.path.join(ypath, mname)\n",
    "            if not os.path.isdir(mpath): \n",
    "                continue\n",
    "\n",
    "            # month dir might be 201808 / 2018_08 / 2018-08 / 08\n",
    "            y2, m2 = parse_month_folder_name(mname)\n",
    "            if y2 is not None:  # matched 201808 / 2018_08 / 2018-08\n",
    "                out.append((mname, y2, m2, mpath))\n",
    "            else:\n",
    "                mm = re.match(r'^(\\d{2})$', mname)  # just \"08\"\n",
    "                if mm:\n",
    "                    m = int(mm.group(1))\n",
    "                    if 1 <= m <= 12:\n",
    "                        out.append((f\"{y:04d}_{m:02d}\", y, m, mpath))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def extract_date_from_filename(fname: str):\n",
    "    \"\"\"\n",
    "    Find token 'sYYYYMMDD' anywhere in the filename.\n",
    "    e.g., cyg08.ddmi.s20240403-000000-e20240403-235959.l1.power-brcs.a31.d32.nc\n",
    "    Returns 'YYYYMMDD' or None.\n",
    "    \"\"\"\n",
    "    m = re.search(r's(\\d{8})', fname)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def open_nc_safely(path: str):\n",
    "    \"\"\"\n",
    "    Open a NetCDF dataset with xarray; caller must close() when done.\n",
    "    Use defaults; if needed, you can specify engine='netcdf4' and mask_and_scale=True.\n",
    "    \"\"\"\n",
    "    return xr.open_dataset(path)  # add engine='netcdf4', mask_and_scale=True if desired\n",
    "\n",
    "\n",
    "def roi_corners(sp_lon, sp_lat, roi_major_km, roi_minor_km, sp_theta_orbit):\n",
    "    \"\"\"\n",
    "    Compute ROI rectangle corners (lon, lat) given center point, size and orientation.\n",
    "    Angle convention: 0 = east, positive counter-clockwise.\n",
    "    \"\"\"\n",
    "    # Convert axis lengths to meters (half-lengths)\n",
    "    a = roi_major_km * 1000 / 2.0\n",
    "    b = roi_minor_km * 1000 / 2.0\n",
    "\n",
    "    # Rotation angle in radians\n",
    "    theta = np.deg2rad(sp_theta_orbit)\n",
    "\n",
    "    # Local meters-to-degrees scale factors\n",
    "    lat_rad = np.deg2rad(sp_lat)\n",
    "    m_per_deg_lat = 111132.92 - 559.82*np.cos(2*lat_rad) + 1.175*np.cos(4*lat_rad)\n",
    "    m_per_deg_lon = 111412.84*np.cos(lat_rad) - 93.5*np.cos(3*lat_rad)\n",
    "\n",
    "    # Rectangle in local ENU coordinates (before rotation)\n",
    "    rect = np.array([\n",
    "        [ a,  b],\n",
    "        [-a,  b],\n",
    "        [-a, -b],\n",
    "        [ a, -b]\n",
    "    ])\n",
    "\n",
    "    # Rotation matrix (theta from east CCW)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta),  np.cos(theta)]\n",
    "    ])\n",
    "    rect_rot = rect @ R.T\n",
    "\n",
    "    # Convert to lon/lat offsets\n",
    "    dlon = rect_rot[:, 0] / m_per_deg_lon\n",
    "    dlat = rect_rot[:, 1] / m_per_deg_lat\n",
    "\n",
    "    corners = [(sp_lon + dlon[i], sp_lat + dlat[i]) for i in range(4)]\n",
    "    return corners\n",
    " \n",
    "def dataset_to_rows(ds):\n",
    "    \"\"\"\n",
    "    Convert required arrays from the dataset to a list of rows.\n",
    "    Each row also includes ROI polygon WKT computed from (sp_lon, sp_lat, sp_theta_orbit).\n",
    "    Returns (rows: list[list], ok: bool). ok=False if any variable missing or shape mismatch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        samples = ds['sample'].values          # (Nsamples,)\n",
    "        ddms = ds['ddm'].values                # (Nddm,)\n",
    "        sp_lon = ds['sp_lon'].values           # (Nsamples, Nddm)\n",
    "        sp_lat = ds['sp_lat'].values           # (Nsamples, Nddm)\n",
    "        ddm_snr = ds['ddm_snr'].values         # (Nsamples, Nddm)\n",
    "        gps_tx_power = ds['gps_tx_power_db_w'].values     # (Nsamples, Nddm)\n",
    "        tx_to_sp = ds['tx_to_sp_range'].values            # (Nsamples, Nddm)\n",
    "        rx_to_sp = ds['rx_to_sp_range'].values            # (Nsamples, Nddm)\n",
    "        sp_inc_angle = ds['sp_inc_angle'].values          # (Nsamples, Nddm)\n",
    "        gps_ant_gain = ds['gps_ant_gain_db_i'].values     # (Nsamples, Nddm)\n",
    "\n",
    "        # ROI orientation (optional)\n",
    "        if \"sp_theta_orbit\" in ds.variables:\n",
    "            sp_theta_orbit = ds['sp_theta_orbit'].values  # (Nsamples, Nddm)\n",
    "        else:\n",
    "            sp_theta_orbit = np.full_like(sp_lon, np.nan)  # fallback if not present\n",
    " \n",
    "    except KeyError as e:\n",
    "        print(f\"  [WARN] Missing variable: {e}\")\n",
    "        return [], False\n",
    " \n",
    "    # Basic shape checks\n",
    "    if (len(samples) != sp_lon.shape[0]) or (len(ddms) != sp_lon.shape[1]):\n",
    "        print(\"  [WARN] Dimension mismatch among variables.\")\n",
    "        return [], False\n",
    " \n",
    "    # Normalize lon to [-180, 180]\n",
    "    sp_lon = np.where(sp_lon > 180, sp_lon - 360, sp_lon)\n",
    " \n",
    "    rows = []\n",
    "    for i in range(len(samples)):\n",
    "        for j in range(len(ddms)):\n",
    "            lon = float(sp_lon[i, j])\n",
    "            lat = float(sp_lat[i, j])\n",
    "            angle = float(sp_theta_orbit[i, j]) if not np.isnan(sp_theta_orbit[i, j]) else np.nan\n",
    "\n",
    "            # NEW: compute 4 corners directly (or NaNs if angle missing)\n",
    "            if np.isnan(angle):\n",
    "                c1 = c2 = c3 = c4 = (np.nan, np.nan)\n",
    "            else:\n",
    "                corners = roi_corners(lon, lat, ROI_MAJOR_KM, ROI_MINOR_KM, angle)\n",
    "                c1, c2, c3, c4 = corners  # each is (lon, lat)\n",
    "\n",
    "            rows.append([\n",
    "                samples[i], ddms[j], lon, lat, ddm_snr[i, j],\n",
    "                gps_tx_power[i, j], tx_to_sp[i, j], rx_to_sp[i, j],\n",
    "                sp_inc_angle[i, j], gps_ant_gain[i, j],\n",
    "                angle, ROI_MAJOR_KM, ROI_MINOR_KM,\n",
    "                c1[0], c1[1], c2[0], c2[1], c3[0], c3[1], c4[0], c4[1]\n",
    "            ])\n",
    "\n",
    "    return rows, True\n",
    " \n",
    " \n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    " \n",
    " \n",
    "# ========= Main =========\n",
    "def main():\n",
    "    ensure_dir(output_root)\n",
    "\n",
    "    month_folders = find_month_dirs(input_root, years_to_process)\n",
    "\n",
    "    if not month_folders:\n",
    "        print(\"[INFO] No month folders matched your year filter.\")\n",
    "        existing = [d for d in sorted(os.listdir(input_root)) if os.path.isdir(os.path.join(input_root, d))]\n",
    "        print(\"[HINT] Top-level dirs under input_root:\", existing)\n",
    "        print(\"[HINT] Expect either month dirs like 201808, or year/mon like 2018/201808 or 2018/08.\")\n",
    "        print(f\"[HINT] Current year filter = {sorted(normalize_years(years_to_process)) if years_to_process else 'None'}\")\n",
    "        return\n",
    "\n",
    "    for name, y, m, full_month_path in month_folders:\n",
    "        print(f\"\\n=== Processing month: {name} ===\")\n",
    "\n",
    "        # Group files by date (YYYYMMDD)\n",
    "        daily_files = defaultdict(list)\n",
    "        for fname in sorted(os.listdir(full_month_path)):\n",
    "            if fname.endswith(\".nc\") and \"cyg\" in fname:\n",
    "                date_str = extract_date_from_filename(fname)\n",
    "                if date_str:\n",
    "                    daily_files[date_str].append(os.path.join(full_month_path, fname))\n",
    "                else:\n",
    "                    print(f\"  [WARN] Could not find sYYYYMMDD in filename: {fname}\")\n",
    "\n",
    "        if not daily_files:\n",
    "            print(\"  [INFO] No NetCDF files found with recognizable dates in this month.\")\n",
    "            continue\n",
    "\n",
    "        # Process each day within this month\n",
    "        for date_str, file_list in tqdm(daily_files.items(), desc=f\"{name}: per-day\"):\n",
    "            daily_rows = []\n",
    "\n",
    "            for nc_file in file_list:\n",
    "                try:\n",
    "                    ds = open_nc_safely(nc_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARN] Failed to open {nc_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    rows, ok = dataset_to_rows(ds)\n",
    "                    if ok:\n",
    "                        daily_rows.extend(rows)\n",
    "                finally:\n",
    "                    try:\n",
    "                        ds.close()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            if not daily_rows:\n",
    "                print(f\"  [INFO] No valid rows for {date_str}\")\n",
    "                continue\n",
    " \n",
    "            # Build DataFrame\n",
    "            df = pd.DataFrame(\n",
    "                daily_rows,\n",
    "                columns=[\n",
    "                    'sample', 'ddm', 'sp_lon', 'sp_lat', 'ddm_snr',\n",
    "                    'gps_tx_power_db_w', 'tx_to_sp_range', 'rx_to_sp_range',\n",
    "                    'sp_inc_angle', 'gps_ant_gain_db_i',\n",
    "                    'sp_theta_orbit', 'roi_major_km', 'roi_minor_km', 'c1_lon','c1_lat','c2_lon','c2_lat','c3_lon','c3_lat','c4_lon','c4_lat'\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Drop full-NaN rows (defensive; usually none after above checks)\n",
    "            df.dropna(how=\"all\", inplace=True)\n",
    "\n",
    "            # Prepare output path (month-by-month)\n",
    "            out_month_folder = os.path.join(output_root, f\"{y:04d}_{m:02d}\")\n",
    "            ensure_dir(out_month_folder)\n",
    "            out_csv = os.path.join(out_month_folder, f\"{date_str}.csv\")\n",
    "\n",
    "            try:\n",
    "                df.to_csv(out_csv, index=False)\n",
    "                # print(f\"  [OK] Saved {out_csv} ({len(df):,} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"  [ERR] Failed to save {out_csv}: {e}\")\n",
    "\n",
    "    print(\"\\n[DONE] All requested years processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954fa07-5116-4c00-81e2-6191edf5c906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
