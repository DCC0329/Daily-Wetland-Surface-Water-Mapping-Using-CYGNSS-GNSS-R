{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1210b0-5766-46ff-8cc8-d1a534345e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2018_08: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [17:32<00:00, 33.96s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2018_09: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [17:50<00:00, 35.70s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2018_10: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [19:30<00:00, 37.75s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2018_11: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [18:45<00:00, 37.50s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2018_12: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [20:22<00:00, 39.42s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Estimated input rows: 403,164,992; kept (land) rows: 108,421,130.\n",
      "Outputs saved under: /mnt/cephfs-mount/chenchen/CygnssDataCsvLand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Filter CYGNSS daily CSVs to land-only using Natural Earth land polygons.\n",
    "\n",
    "import os, re, sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------- User paths -----------------\n",
    "INPUT_ROOT  = r\"/mnt/cephfs-mount/chenchen/CygnssDataCsv\"\n",
    "OUTPUT_ROOT = r\"/mnt/cephfs-mount/chenchen/CygnssDataCsvLand\"\n",
    "\n",
    "# Only process these years; set to None to process all\n",
    "YEARS_TO_PROCESS = {2018}   # e.g. {2018} or set(range(2019, 2022)) or None\n",
    "\n",
    "# Skip Antarctica to avoid coastal shelf artifacts (set False to keep)\n",
    "DROP_ANTARCTICA = True\n",
    "\n",
    "# If True, still write header-only CSVs when no land points; if False, skip writing\n",
    "WRITE_EMPTY_FILES = True\n",
    "\n",
    "# Natural Earth Admin-0 countries shapefile (50m = better coasts than 110m).\n",
    "# You can also point this to a local .zip on disk.\n",
    "NE_ADMIN0_SOURCE = os.environ.get(\n",
    "    \"NE_ADMIN0_SOURCE\",\n",
    "    \"https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_countries.zip\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Robust CSV reader that handles weird delimiters/encodings.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"c\", low_memory=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    for kwargs in (\n",
    "        dict(engine=\"python\", sep=None, low_memory=False),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", low_memory=False),\n",
    "    ):\n",
    "        try:\n",
    "            return pd.read_csv(path, **kwargs)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last-resort encodings\n",
    "    for enc in (\"utf-8\", \"latin1\", \"utf-16\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", sep=None, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Unable to read CSV: {path}\")\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def load_land_gdf():\n",
    "    \"\"\"\n",
    "    Load land polygons using Natural Earth Admin-0 countries.\n",
    "    Drops Antarctica if requested, then keeps only the geometry column.\n",
    "    Works with GeoPandas >=1.0 (no deprecated datasets API).\n",
    "    \"\"\"\n",
    "    # Read from URL or local .zip path\n",
    "    countries = gpd.read_file(NE_ADMIN0_SOURCE)\n",
    "\n",
    "    # Drop Antarctica if requested (robust to different name columns)\n",
    "    if DROP_ANTARCTICA:\n",
    "        name_cols = [\"NAME_EN\", \"NAME\", \"ADMIN\", \"name_en\", \"name\", \"NAME_LONG\", \"name_long\"]\n",
    "        ant_col = next((c for c in name_cols if c in countries.columns), None)\n",
    "        if ant_col is not None:\n",
    "            countries = countries[countries[ant_col] != \"Antarctica\"]\n",
    "        else:\n",
    "            # Fallback: filter by latitude (remove features extending below ~60°S)\n",
    "            b = countries.bounds  # DataFrame with minx, miny, maxx, maxy\n",
    "            countries = countries[b[\"miny\"] > -60]\n",
    "\n",
    "    # Keep WGS84 and only geometry (no dissolve needed; sjoin handles multiple polygons)\n",
    "    land = countries.to_crs(\"EPSG:4326\")[[\"geometry\"]].reset_index(drop=True)\n",
    "\n",
    "    # Build spatial index once (speeds up sjoin on large batches)\n",
    "    _ = land.sindex\n",
    "    return land\n",
    "\n",
    "\n",
    "def sjoin_points_on_land(points_gdf, land_gdf):\n",
    "    \"\"\"Spatial join for points within land polygons (handles older geopandas API too).\"\"\"\n",
    "    try:\n",
    "        hits = gpd.sjoin(points_gdf, land_gdf, how=\"inner\", predicate=\"within\")\n",
    "    except TypeError:\n",
    "        hits = gpd.sjoin(points_gdf, land_gdf, how=\"inner\", op=\"within\")\n",
    "    # We only need the point indices\n",
    "    return hits.index.unique()\n",
    "\n",
    "def process_csv(in_csv, out_csv, land_gdf):\n",
    "    df = safe_read_csv(in_csv)\n",
    "\n",
    "    # Require columns\n",
    "    if not {'sp_lon','sp_lat'}.issubset(df.columns):\n",
    "        print(f\"[WARN] Missing sp_lon/sp_lat in {in_csv}; copying header only.\")\n",
    "        ensure_dir(os.path.dirname(out_csv))\n",
    "        df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Coerce numeric and drop NaNs\n",
    "    df['sp_lon'] = pd.to_numeric(df['sp_lon'], errors='coerce')\n",
    "    df['sp_lat'] = pd.to_numeric(df['sp_lat'], errors='coerce')\n",
    "    df2 = df.dropna(subset=['sp_lon','sp_lat'])\n",
    "\n",
    "    if df2.empty:\n",
    "        if WRITE_EMPTY_FILES:\n",
    "            ensure_dir(os.path.dirname(out_csv))\n",
    "            df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Build point GeoDataFrame in WGS84\n",
    "    points = gpd.GeoDataFrame(\n",
    "        df2[['sp_lon','sp_lat']],\n",
    "        geometry=gpd.points_from_xy(df2['sp_lon'], df2['sp_lat']),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    keep_idx = sjoin_points_on_land(points, land_gdf)\n",
    "\n",
    "    if len(keep_idx) == 0:\n",
    "        if WRITE_EMPTY_FILES:\n",
    "            ensure_dir(os.path.dirname(out_csv))\n",
    "            df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Map back to original rows (preserve original column order/types)\n",
    "    df_out = df.loc[keep_idx]\n",
    "    ensure_dir(os.path.dirname(out_csv))\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    return len(df_out)\n",
    "\n",
    "def main():\n",
    "    land_gdf = load_land_gdf()\n",
    "\n",
    "    # Find folders like YYYY_MM, optionally filtered by year\n",
    "    month_dirs = []\n",
    "    for d in sorted(os.listdir(INPUT_ROOT)):\n",
    "        p = os.path.join(INPUT_ROOT, d)\n",
    "        if not (os.path.isdir(p) and re.fullmatch(r\"\\d{4}_\\d{2}\", d)):\n",
    "            continue\n",
    "        year = int(d[:4])\n",
    "        if YEARS_TO_PROCESS is not None and year not in YEARS_TO_PROCESS:\n",
    "            continue\n",
    "        month_dirs.append(d)\n",
    "\n",
    "\n",
    "    total_in, total_kept = 0, 0\n",
    "\n",
    "    for month in month_dirs:\n",
    "        in_month  = os.path.join(INPUT_ROOT,  month)\n",
    "        out_month = os.path.join(OUTPUT_ROOT, month)\n",
    "        ensure_dir(out_month)\n",
    "\n",
    "        csvs = [f for f in sorted(os.listdir(in_month)) if f.lower().endswith(\".csv\")]\n",
    "        if not csvs:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {month}: {len(csvs)} files\")\n",
    "        for fname in tqdm(csvs, unit=\"file\"):\n",
    "            in_csv  = os.path.join(in_month, fname)\n",
    "            out_csv = os.path.join(out_month, fname)\n",
    "            try:\n",
    "                # Count rows before filtering (for a quick summary)\n",
    "                try:\n",
    "                    n_before = sum(1 for _ in open(in_csv, 'r', encoding='utf-8', errors='ignore')) - 1\n",
    "                    if n_before < 0: n_before = 0\n",
    "                except Exception:\n",
    "                    n_before = 0\n",
    "\n",
    "                kept = process_csv(in_csv, out_csv, land_gdf)\n",
    "                total_in   += max(n_before, kept)  # best-effort accounting\n",
    "                total_kept += kept\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {in_csv}: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "    print(f\"\\nDone. Estimated input rows: {total_in:,}; kept (land) rows: {total_kept:,}.\")\n",
    "    print(f\"Outputs saved under: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc90f32-36ad-4865-8a42-7df202d5f960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_01: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [18:44<00:00, 36.26s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_02: 28 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 28/28 [16:42<00:00, 35.79s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_03: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [18:39<00:00, 36.13s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_04: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [18:20<00:00, 36.67s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_05: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [19:29<00:00, 37.71s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_06: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [19:47<00:00, 39.59s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_07: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [43:01<00:00, 83.27s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_08: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [44:12<00:00, 85.55s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_09: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [41:56<00:00, 83.87s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_10: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [42:38<00:00, 82.53s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_11: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [42:17<00:00, 84.59s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2019_12: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [41:58<00:00, 81.23s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_01: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [38:42<00:00, 74.93s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_02: 29 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 29/29 [34:48<00:00, 72.01s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_03: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [38:49<00:00, 75.15s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_04: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [40:41<00:00, 81.37s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_05: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [42:53<00:00, 83.01s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_06: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [40:40<00:00, 81.36s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_07: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [39:01<00:00, 75.54s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_08: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [39:54<00:00, 77.26s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_09: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [40:59<00:00, 81.97s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_10: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [40:45<00:00, 78.89s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_11: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [39:39<00:00, 79.32s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2020_12: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [38:11<00:00, 73.91s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_01: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [39:04<00:00, 75.63s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_02: 28 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 28/28 [36:45<00:00, 78.78s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_03: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [40:13<00:00, 77.86s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_04: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [36:36<00:00, 73.21s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_05: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [37:28<00:00, 72.52s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_06: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [36:45<00:00, 73.53s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_07: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [38:48<00:00, 75.10s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_08: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [39:10<00:00, 75.81s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_09: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [44:36<00:00, 89.23s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_10: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [51:38<00:00, 99.95s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_11: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [36:35<00:00, 73.18s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2021_12: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [40:13<00:00, 77.85s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_01: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [37:11<00:00, 72.00s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_02: 28 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 28/28 [36:37<00:00, 78.49s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_03: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [39:13<00:00, 75.93s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_04: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [36:48<00:00, 73.60s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_05: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [37:57<00:00, 73.47s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_06: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [38:14<00:00, 76.47s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_07: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [39:27<00:00, 76.37s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_08: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [42:01<00:00, 81.33s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_09: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [43:36<00:00, 87.23s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_10: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [43:36<00:00, 84.39s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_11: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [39:46<00:00, 79.56s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2022_12: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [34:45<00:00, 67.27s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_01: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [34:09<00:00, 66.10s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_02: 28 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 28/28 [29:57<00:00, 64.19s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_03: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [35:02<00:00, 67.83s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_04: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [33:23<00:00, 66.78s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_05: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [35:21<00:00, 68.45s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_06: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [35:19<00:00, 70.66s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_07: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [35:22<00:00, 68.48s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_08: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [38:59<00:00, 75.46s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_09: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [36:03<00:00, 72.13s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_10: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [34:22<00:00, 66.53s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_11: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [35:17<00:00, 70.60s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2023_12: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [35:29<00:00, 68.70s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Estimated input rows: 9,213,776,708; kept (land) rows: 2,482,978,703.\n",
      "Outputs saved under: /mnt/cephfs-mount/chenchen/CygnssDataCsvLand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Filter CYGNSS daily CSVs to land-only using Natural Earth land polygons.\n",
    "\n",
    "import os, re, sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------- User paths -----------------\n",
    "INPUT_ROOT  = r\"/mnt/cephfs-mount/chenchen/CygnssDataCsv\"\n",
    "OUTPUT_ROOT = r\"/mnt/cephfs-mount/chenchen/CygnssDataCsvLand\"\n",
    "\n",
    "# Only process these years; set to None to process all\n",
    "YEARS_TO_PROCESS = {2019,2020,2021,2022,2023}   # e.g. {2019,2020,2021,2022,2023} or set(range(2019, 2022)) or None\n",
    "\n",
    "# Skip Antarctica to avoid coastal shelf artifacts (set False to keep)\n",
    "DROP_ANTARCTICA = True\n",
    "\n",
    "# If True, still write header-only CSVs when no land points; if False, skip writing\n",
    "WRITE_EMPTY_FILES = True\n",
    "\n",
    "# Natural Earth Admin-0 countries shapefile (50m = better coasts than 110m).\n",
    "# You can also point this to a local .zip on disk.\n",
    "NE_ADMIN0_SOURCE = os.environ.get(\n",
    "    \"NE_ADMIN0_SOURCE\",\n",
    "    \"https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_countries.zip\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Robust CSV reader that handles weird delimiters/encodings.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"c\", low_memory=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    for kwargs in (\n",
    "        dict(engine=\"python\", sep=None, low_memory=False),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", low_memory=False),\n",
    "    ):\n",
    "        try:\n",
    "            return pd.read_csv(path, **kwargs)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last-resort encodings\n",
    "    for enc in (\"utf-8\", \"latin1\", \"utf-16\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", sep=None, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Unable to read CSV: {path}\")\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def load_land_gdf():\n",
    "    \"\"\"\n",
    "    Load land polygons using Natural Earth Admin-0 countries.\n",
    "    Drops Antarctica if requested, then keeps only the geometry column.\n",
    "    Works with GeoPandas >=1.0 (no deprecated datasets API).\n",
    "    \"\"\"\n",
    "    # Read from URL or local .zip path\n",
    "    countries = gpd.read_file(NE_ADMIN0_SOURCE)\n",
    "\n",
    "    # Drop Antarctica if requested (robust to different name columns)\n",
    "    if DROP_ANTARCTICA:\n",
    "        name_cols = [\"NAME_EN\", \"NAME\", \"ADMIN\", \"name_en\", \"name\", \"NAME_LONG\", \"name_long\"]\n",
    "        ant_col = next((c for c in name_cols if c in countries.columns), None)\n",
    "        if ant_col is not None:\n",
    "            countries = countries[countries[ant_col] != \"Antarctica\"]\n",
    "        else:\n",
    "            # Fallback: filter by latitude (remove features extending below ~60°S)\n",
    "            b = countries.bounds  # DataFrame with minx, miny, maxx, maxy\n",
    "            countries = countries[b[\"miny\"] > -60]\n",
    "\n",
    "    # Keep WGS84 and only geometry (no dissolve needed; sjoin handles multiple polygons)\n",
    "    land = countries.to_crs(\"EPSG:4326\")[[\"geometry\"]].reset_index(drop=True)\n",
    "\n",
    "    # Build spatial index once (speeds up sjoin on large batches)\n",
    "    _ = land.sindex\n",
    "    return land\n",
    "\n",
    "\n",
    "def sjoin_points_on_land(points_gdf, land_gdf):\n",
    "    \"\"\"Spatial join for points within land polygons (handles older geopandas API too).\"\"\"\n",
    "    try:\n",
    "        hits = gpd.sjoin(points_gdf, land_gdf, how=\"inner\", predicate=\"within\")\n",
    "    except TypeError:\n",
    "        hits = gpd.sjoin(points_gdf, land_gdf, how=\"inner\", op=\"within\")\n",
    "    # We only need the point indices\n",
    "    return hits.index.unique()\n",
    "\n",
    "def process_csv(in_csv, out_csv, land_gdf):\n",
    "    df = safe_read_csv(in_csv)\n",
    "\n",
    "    # Require columns\n",
    "    if not {'sp_lon','sp_lat'}.issubset(df.columns):\n",
    "        print(f\"[WARN] Missing sp_lon/sp_lat in {in_csv}; copying header only.\")\n",
    "        ensure_dir(os.path.dirname(out_csv))\n",
    "        df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Coerce numeric and drop NaNs\n",
    "    df['sp_lon'] = pd.to_numeric(df['sp_lon'], errors='coerce')\n",
    "    df['sp_lat'] = pd.to_numeric(df['sp_lat'], errors='coerce')\n",
    "    df2 = df.dropna(subset=['sp_lon','sp_lat'])\n",
    "\n",
    "    if df2.empty:\n",
    "        if WRITE_EMPTY_FILES:\n",
    "            ensure_dir(os.path.dirname(out_csv))\n",
    "            df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Build point GeoDataFrame in WGS84\n",
    "    points = gpd.GeoDataFrame(\n",
    "        df2[['sp_lon','sp_lat']],\n",
    "        geometry=gpd.points_from_xy(df2['sp_lon'], df2['sp_lat']),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    keep_idx = sjoin_points_on_land(points, land_gdf)\n",
    "\n",
    "    if len(keep_idx) == 0:\n",
    "        if WRITE_EMPTY_FILES:\n",
    "            ensure_dir(os.path.dirname(out_csv))\n",
    "            df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Map back to original rows (preserve original column order/types)\n",
    "    df_out = df.loc[keep_idx]\n",
    "    ensure_dir(os.path.dirname(out_csv))\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    return len(df_out)\n",
    "\n",
    "def main():\n",
    "    land_gdf = load_land_gdf()\n",
    "\n",
    "    # Find folders like YYYY_MM, optionally filtered by year\n",
    "    month_dirs = []\n",
    "    for d in sorted(os.listdir(INPUT_ROOT)):\n",
    "        p = os.path.join(INPUT_ROOT, d)\n",
    "        if not (os.path.isdir(p) and re.fullmatch(r\"\\d{4}_\\d{2}\", d)):\n",
    "            continue\n",
    "        year = int(d[:4])\n",
    "        if YEARS_TO_PROCESS is not None and year not in YEARS_TO_PROCESS:\n",
    "            continue\n",
    "        month_dirs.append(d)\n",
    "\n",
    "\n",
    "    total_in, total_kept = 0, 0\n",
    "\n",
    "    for month in month_dirs:\n",
    "        in_month  = os.path.join(INPUT_ROOT,  month)\n",
    "        out_month = os.path.join(OUTPUT_ROOT, month)\n",
    "        ensure_dir(out_month)\n",
    "\n",
    "        csvs = [f for f in sorted(os.listdir(in_month)) if f.lower().endswith(\".csv\")]\n",
    "        if not csvs:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {month}: {len(csvs)} files\")\n",
    "        for fname in tqdm(csvs, unit=\"file\"):\n",
    "            in_csv  = os.path.join(in_month, fname)\n",
    "            out_csv = os.path.join(out_month, fname)\n",
    "            try:\n",
    "                # Count rows before filtering (for a quick summary)\n",
    "                try:\n",
    "                    n_before = sum(1 for _ in open(in_csv, 'r', encoding='utf-8', errors='ignore')) - 1\n",
    "                    if n_before < 0: n_before = 0\n",
    "                except Exception:\n",
    "                    n_before = 0\n",
    "\n",
    "                kept = process_csv(in_csv, out_csv, land_gdf)\n",
    "                total_in   += max(n_before, kept)  # best-effort accounting\n",
    "                total_kept += kept\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {in_csv}: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "    print(f\"\\nDone. Estimated input rows: {total_in:,}; kept (land) rows: {total_kept:,}.\")\n",
    "    print(f\"Outputs saved under: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cdf5b6-cabd-467e-8d44-e0be09503d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_01: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [31:09<00:00, 60.30s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_02: 28 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 28/28 [32:31<00:00, 69.71s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_03: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [37:40<00:00, 72.93s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_04: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [29:17<00:00, 58.58s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_05: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [30:31<00:00, 59.07s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_06: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [28:16<00:00, 56.55s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2025_07: 31 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 31/31 [29:07<00:00, 56.39s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Estimated input rows: 998,920,888; kept (land) rows: 266,596,848.\n",
      "Outputs saved under: /mnt/cephfs-mount/chenchen/CygnssDataCsvLand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Filter CYGNSS daily CSVs to land-only using Natural Earth land polygons.\n",
    "\n",
    "import os, re, sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------- User paths -----------------\n",
    "INPUT_ROOT  = r\"/mnt/cephfs-mount/chenchen/CygnssDataCsv\"\n",
    "OUTPUT_ROOT = r\"/mnt/cephfs-mount/chenchen/CygnssDataCsvLand\"\n",
    "\n",
    "# Only process these years; set to None to process all\n",
    "YEARS_TO_PROCESS = {2025}   # e.g. {2019,2020,2021,2022,2023} or set(range(2019, 2022)) or None\n",
    "\n",
    "# Skip Antarctica to avoid coastal shelf artifacts (set False to keep)\n",
    "DROP_ANTARCTICA = True\n",
    "\n",
    "# If True, still write header-only CSVs when no land points; if False, skip writing\n",
    "WRITE_EMPTY_FILES = True\n",
    "\n",
    "# Natural Earth Admin-0 countries shapefile (50m = better coasts than 110m).\n",
    "# You can also point this to a local .zip on disk.\n",
    "NE_ADMIN0_SOURCE = os.environ.get(\n",
    "    \"NE_ADMIN0_SOURCE\",\n",
    "    \"https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_countries.zip\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Robust CSV reader that handles weird delimiters/encodings.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"c\", low_memory=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    for kwargs in (\n",
    "        dict(engine=\"python\", sep=None, low_memory=False),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", low_memory=False),\n",
    "    ):\n",
    "        try:\n",
    "            return pd.read_csv(path, **kwargs)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last-resort encodings\n",
    "    for enc in (\"utf-8\", \"latin1\", \"utf-16\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", sep=None, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Unable to read CSV: {path}\")\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def load_land_gdf():\n",
    "    \"\"\"\n",
    "    Load land polygons using Natural Earth Admin-0 countries.\n",
    "    Drops Antarctica if requested, then keeps only the geometry column.\n",
    "    Works with GeoPandas >=1.0 (no deprecated datasets API).\n",
    "    \"\"\"\n",
    "    # Read from URL or local .zip path\n",
    "    countries = gpd.read_file(NE_ADMIN0_SOURCE)\n",
    "\n",
    "    # Drop Antarctica if requested (robust to different name columns)\n",
    "    if DROP_ANTARCTICA:\n",
    "        name_cols = [\"NAME_EN\", \"NAME\", \"ADMIN\", \"name_en\", \"name\", \"NAME_LONG\", \"name_long\"]\n",
    "        ant_col = next((c for c in name_cols if c in countries.columns), None)\n",
    "        if ant_col is not None:\n",
    "            countries = countries[countries[ant_col] != \"Antarctica\"]\n",
    "        else:\n",
    "            # Fallback: filter by latitude (remove features extending below ~60°S)\n",
    "            b = countries.bounds  # DataFrame with minx, miny, maxx, maxy\n",
    "            countries = countries[b[\"miny\"] > -60]\n",
    "\n",
    "    # Keep WGS84 and only geometry (no dissolve needed; sjoin handles multiple polygons)\n",
    "    land = countries.to_crs(\"EPSG:4326\")[[\"geometry\"]].reset_index(drop=True)\n",
    "\n",
    "    # Build spatial index once (speeds up sjoin on large batches)\n",
    "    _ = land.sindex\n",
    "    return land\n",
    "\n",
    "\n",
    "def sjoin_points_on_land(points_gdf, land_gdf):\n",
    "    \"\"\"Spatial join for points within land polygons (handles older geopandas API too).\"\"\"\n",
    "    try:\n",
    "        hits = gpd.sjoin(points_gdf, land_gdf, how=\"inner\", predicate=\"within\")\n",
    "    except TypeError:\n",
    "        hits = gpd.sjoin(points_gdf, land_gdf, how=\"inner\", op=\"within\")\n",
    "    # We only need the point indices\n",
    "    return hits.index.unique()\n",
    "\n",
    "def process_csv(in_csv, out_csv, land_gdf):\n",
    "    df = safe_read_csv(in_csv)\n",
    "\n",
    "    # Require columns\n",
    "    if not {'sp_lon','sp_lat'}.issubset(df.columns):\n",
    "        print(f\"[WARN] Missing sp_lon/sp_lat in {in_csv}; copying header only.\")\n",
    "        ensure_dir(os.path.dirname(out_csv))\n",
    "        df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Coerce numeric and drop NaNs\n",
    "    df['sp_lon'] = pd.to_numeric(df['sp_lon'], errors='coerce')\n",
    "    df['sp_lat'] = pd.to_numeric(df['sp_lat'], errors='coerce')\n",
    "    df2 = df.dropna(subset=['sp_lon','sp_lat'])\n",
    "\n",
    "    if df2.empty:\n",
    "        if WRITE_EMPTY_FILES:\n",
    "            ensure_dir(os.path.dirname(out_csv))\n",
    "            df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Build point GeoDataFrame in WGS84\n",
    "    points = gpd.GeoDataFrame(\n",
    "        df2[['sp_lon','sp_lat']],\n",
    "        geometry=gpd.points_from_xy(df2['sp_lon'], df2['sp_lat']),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    keep_idx = sjoin_points_on_land(points, land_gdf)\n",
    "\n",
    "    if len(keep_idx) == 0:\n",
    "        if WRITE_EMPTY_FILES:\n",
    "            ensure_dir(os.path.dirname(out_csv))\n",
    "            df.head(0).to_csv(out_csv, index=False)\n",
    "        return 0\n",
    "\n",
    "    # Map back to original rows (preserve original column order/types)\n",
    "    df_out = df.loc[keep_idx]\n",
    "    ensure_dir(os.path.dirname(out_csv))\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    return len(df_out)\n",
    "\n",
    "def main():\n",
    "    land_gdf = load_land_gdf()\n",
    "\n",
    "    # Find folders like YYYY_MM, optionally filtered by year\n",
    "    month_dirs = []\n",
    "    for d in sorted(os.listdir(INPUT_ROOT)):\n",
    "        p = os.path.join(INPUT_ROOT, d)\n",
    "        if not (os.path.isdir(p) and re.fullmatch(r\"\\d{4}_\\d{2}\", d)):\n",
    "            continue\n",
    "        year = int(d[:4])\n",
    "        if YEARS_TO_PROCESS is not None and year not in YEARS_TO_PROCESS:\n",
    "            continue\n",
    "        month_dirs.append(d)\n",
    "\n",
    "\n",
    "    total_in, total_kept = 0, 0\n",
    "\n",
    "    for month in month_dirs:\n",
    "        in_month  = os.path.join(INPUT_ROOT,  month)\n",
    "        out_month = os.path.join(OUTPUT_ROOT, month)\n",
    "        ensure_dir(out_month)\n",
    "\n",
    "        csvs = [f for f in sorted(os.listdir(in_month)) if f.lower().endswith(\".csv\")]\n",
    "        if not csvs:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {month}: {len(csvs)} files\")\n",
    "        for fname in tqdm(csvs, unit=\"file\"):\n",
    "            in_csv  = os.path.join(in_month, fname)\n",
    "            out_csv = os.path.join(out_month, fname)\n",
    "            try:\n",
    "                # Count rows before filtering (for a quick summary)\n",
    "                try:\n",
    "                    n_before = sum(1 for _ in open(in_csv, 'r', encoding='utf-8', errors='ignore')) - 1\n",
    "                    if n_before < 0: n_before = 0\n",
    "                except Exception:\n",
    "                    n_before = 0\n",
    "\n",
    "                kept = process_csv(in_csv, out_csv, land_gdf)\n",
    "                total_in   += max(n_before, kept)  # best-effort accounting\n",
    "                total_kept += kept\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {in_csv}: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "    print(f\"\\nDone. Estimated input rows: {total_in:,}; kept (land) rows: {total_kept:,}.\")\n",
    "    print(f\"Outputs saved under: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
